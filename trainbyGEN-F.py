'''
Train the classifier network by
samples generated by feature generator
'''

import argparse
import torch.nn as nn
from torch.utils.data import DataLoader

from modelsMNIST.GAN import *
from modelsMNIST.VAE import *
from modelsMNIST.DDPM import *
from models.mlp import *

from generators16.DCGAN import *
from generators16.CCVAE import *

from models.cnn import *

from utils.util import test_img
from utils.getData import *
import copy

import wandb
from datetime import datetime

parser = argparse.ArgumentParser()
parser.add_argument("--n_fepochs", type=int, default=30, help="number of epochs of training by generated samples")
parser.add_argument("--n_epochs", type=int, default=50, help="number of epochs of training by local samples")
parser.add_argument("--batch_size", type=int, default=32, help="size of the batches")
parser.add_argument("--bs", type=int, default=128, help="size of the batches")
parser.add_argument('--local_bs', type=int, default=128)

parser.add_argument('--device_id', type=str, default='0')

parser.add_argument('--dataset', type=str, default='mnist') # stl10, cifar10, svhn, mnist
parser.add_argument('--models', type=str, default='cnn') # cnn, mlp
parser.add_argument("--num_classes", type=int, default=10, help="number of classes for dataset")

parser.add_argument("--img_size", type=int, default=16, help="size of each image dimension")
parser.add_argument("--output_channel", type=int, default=3, help="number of image channels")

parser.add_argument("--n_cpu", type=int, default=8, help="number of cpu threads to use during batch generation")

### generators
parser.add_argument('--gen', type=str, default='dcgan') # dcgan, cave, gan, vae, ddpm
parser.add_argument("--latent_dim", type=int, default=100, help="dimensionality of the latent space") # GAN
parser.add_argument("--latent_size", type=int, default=20, help="dimensionality of the latent space") # VAE
parser.add_argument("--n_feat", type=int, default=128) # DDPM
parser.add_argument("--n_T", type=int, default=5) # DDPM
### optimizer
parser.add_argument('--momentum', type=float, default=0)
parser.add_argument('--weight_decay', type=float, default=0)
### logging
parser.add_argument('--name', type=str, default='default') # dcgan, vae, ddpm
parser.add_argument('--wandb', type=bool, default=False)
### experminets
parser.add_argument('--rs', type=int, default=0)
parser.add_argument("--sample_interval", type=int, default=20, help="interval between image sampling")

args = parser.parse_args()
args.device = 'cuda:' + args.device_id
args.img_shape = (args.output_channel, args.img_size, args.img_size)

torch.manual_seed(args.rs)
torch.cuda.manual_seed(args.rs)
np.random.seed(args.rs)
random.seed(args.rs)

lr = 1e-1 # MLP

timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
filename = './output/gefl/'+ timestamp + str(args.name) + str(args.rs)
if not os.path.exists(filename):
    os.makedirs(filename)

if args.wandb:
    run = wandb.init(dir=filename, project='GeFL-0928', name= str(args.name)+ str(args.rs), reinit=True, settings=wandb.Settings(code_dir="."))
    wandb.config.update(args)

if args.gen == 'gan':
    gennet = Generator(args).to(args.device)
    gennet.load_state_dict(torch.load('models/save/F1_400gan_generator.pt'))
elif args.gen == 'vae':
    gennet = CVAE(args).to(args.device)
    gennet.load_state_dict(torch.load('models/save/F1_400cvae.pt'))
elif args.gen == 'ddpm':
    gennet = DDPM(nn_model=ContextUnet(in_channels=1, n_feat=args.n_feat, n_classes=args.num_classes), betas=(1e-4, 0.02), n_T=400, device=args.device, drop_prob=0.1).to(args.device)
    gennet.load_state_dict(torch.load('models/save/1_100_ddpm.pt'))
elif args.gen == 'dcgan':
    gennet = generator(args).to(args.device)
    gennet.load_state_dict(torch.load('models/save/F10_400dcgan_generator.pt'))
elif args.gen == 'cvae':
    args.latent_size=16
    gennet = CCVAE(args).to(args.device)
    gennet.load_state_dict(torch.load('models/save/F10_400cvae.pt'))

if args.models == 'mlp':
    common_net = FE_MLP().to(args.device)
    net = MLP3().to(args.device)
    w_comm = torch.load('models/save/common_net.pt') # common_net = FE_MLP.to(args.device)
elif args.models == 'cnn':
    common_net = FE_CNN().to(args.device)
    net = CNN3().to(args.device)
    w_comm = torch.load('models/save/0.1_100CNN_common_net.pt') # common_net = FE_MLP.to(args.device)

w_net = net.state_dict()

for key in w_comm:
    w_net[key] = w_comm[key]
net.load_state_dict(w_net)
common_net.load_state_dict(w_comm)
common_net.eval()

loss_func = nn.CrossEntropyLoss()
dataset_train, dataset_test = getDataset(args)

optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=args.momentum, weight_decay=args.weight_decay)
batch_iter = len(dataset_train)//args.batch_size # MNIST

gen_epoch_loss = []
gennet.eval()
for iter in range(1, args.n_fepochs+1): # train by samples generated by generator
    net.train()
    gen_batch_loss = []

    for i in range(batch_iter):
        with torch.no_grad():
            images, labels = gennet.sample_image(args) # images.shape (bs, feature^2)
        net.zero_grad()
        if args.models=='mlp':
            logits, log_probs = net(images.view(-1,args.img_size**2), start_layer='feature')
        elif args.models =='cnn':
            logits, log_probs = net(images, start_layer='feature')
        loss = F.cross_entropy(logits, labels) # net.fc1.weight.grad / net.fc5.weight.grad
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        gen_batch_loss.append(loss.item())
    gen_epoch_loss.append(sum(gen_batch_loss)/len(gen_batch_loss))
    print(iter, 'Epoch loss: {:.4f}'.format(gen_epoch_loss[-1]))

    if iter % 10 == 0 or iter == args.n_fepochs or iter == 1:
        net.eval()
        acc_test, loss_test = test_img(copy.deepcopy(net), dataset_test, args)
        # if acc_test > best_perf[i]:
        #     best_perf[i] = float(acc_test)            
        print("(FedGEN-F) Testing accuracy " + str(iter) + ": {:.2f}".format(acc_test))
        if args.wandb:
            wandb.log({
                "Communication round": iter,
                "Test accuracy": acc_test
            })
try:
    gen_loss = sum(gen_epoch_loss) / len(gen_epoch_loss)
except:
    gen_loss = None
    
dict_users = dict_iid(dataset_train, 100)
ldr_train = DataLoader(DatasetSplit(dataset_train, dict_users[0]), batch_size=args.batch_size, shuffle=True)

optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=args.momentum, weight_decay=args.weight_decay)
epoch_loss = []
for iter in range(1,args.n_epochs+1): # train by samples generated by generator
    net.train()
    batch_loss = []

    for batch_idx, (images, labels) in enumerate(ldr_train):
        images, labels = images.to(args.device), labels.to(args.device)
        with torch.no_grad():
            images = common_net(images)
        net.zero_grad()
        if args.models=='mlp':
            logits, log_probs = net(images.view(-1,args.img_size**2), start_layer='feature')
        elif args.models =='cnn':
            logits, log_probs = net(images, start_layer='feature')
        loss = F.cross_entropy(logits, labels) # net.fc1.weight.grad / net.fc5.weight.grad
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        batch_loss.append(loss.item())
    epoch_loss.append(sum(batch_loss)/len(batch_loss))
    print(iter, 'Epoch loss: {:.4f}'.format(epoch_loss[-1]))

    if iter % 10 == 0 or iter == args.n_epochs:
        net.eval()
        acc_test, loss_test = test_img(copy.deepcopy(net), dataset_test, args)
        # if acc_test > best_perf[i]:
        #     best_perf[i] = float(acc_test)            
        print("Testing accuracy " + str(iter) + ": {:.2f}".format(acc_test))
        if args.wandb:
            wandb.log({
                "Communication round": args.n_fepochs+iter,
                "Test accuracy": acc_test
            })